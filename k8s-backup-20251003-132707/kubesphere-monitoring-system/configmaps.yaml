apiVersion: v1
items:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    annotations:
      control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"notification-manager-operator-9949f4878-x9vqw_9bd69951-a010-45db-8da2-e1e9342d713e","leaseDurationSeconds":15,"acquireTime":"2025-06-20T20:32:08Z","renewTime":"2025-10-03T07:58:52Z","leaderTransitions":15}'
    creationTimestamp: "2025-02-10T07:03:19Z"
    name: 7b8d27e6.kubesphere.io
    namespace: kubesphere-monitoring-system
    resourceVersion: "122980294"
    uid: d79c881e-6590-42c1-8792-fe8aae834de3
- apiVersion: v1
  data:
    ca.crt: |
      -----BEGIN CERTIFICATE-----
      MIIDBTCCAe2gAwIBAgIIBGpEgb8zVRswDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
      AxMKa3ViZXJuZXRlczAeFw0yNTAyMDcwNzU0MDZaFw0zNTAyMDUwNzU5MDZaMBUx
      EzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK
      AoIBAQDcKXkUtHMKM7qeci7OlLNez2FlydMAt9l6KKkywvtuLyaMIrJKbFPh8hml
      H2dnMm7PRXcy/xoZFucFHfjaUY+euYJ6cf3MAoQfG/SfgpA5TRqEPbdxwc+GHDy7
      6ZoH9gIYHLkdeRUw0QTKKZTNYmRhRZ7oLjRmssL0kSpWOrg3EdN/K95ZT4eczUrl
      ts/rLLJPILP6LuWGllkIqnKBpm95RbpeGfTCoCAYbYIpbm0XzbybHeJuar+mA2kG
      Vhv8LJH9HIUCPTPLY4eUNFM5yAOlqR4p2tKpIHcaQC8WZhJqQBd3/OLx1fDnEsu5
      KCg99V39QHNmXONGKNdVZdnvCa8DAgMBAAGjWTBXMA4GA1UdDwEB/wQEAwICpDAP
      BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRSbsFhjuaavB3D2AJAgY6/Kg93NDAV
      BgNVHREEDjAMggprdWJlcm5ldGVzMA0GCSqGSIb3DQEBCwUAA4IBAQBtFumnVigF
      ZqgHe3ok8jdx/o48sBfKM+N+eiqEKsuVuWhiYcMGhlXqqvgX5qHDcM+QPC2x4QxE
      NiZ0beIXwVs/xeOx9KeoPfnem58Ke51qlCBiTZrPTFhCIGUfqpD9SAxFRXX/4Re+
      ff8hORHxXHM+mo9fRUIsDkEON3IoFbImuq0bfklHbPfFJ1z+f/Fg+3bG4bUcMRDG
      /N3BorYqcnQxsvXlZpw3yDQrm3VukCTZSKm3FAuakJlBntmxHM4UewnN0VaF8ZAR
      dfz/NBVYyrEu/zPNPOYYHVLLfcMPbtlxnCnQuBB5C6Bv0hH1YNcwvEObxZH4+8G0
      O6YN95NGdZMw
      -----END CERTIFICATE-----
  kind: ConfigMap
  metadata:
    annotations:
      kubernetes.io/description: Contains a CA bundle that can be used to verify the
        kube-apiserver when using internal endpoints such as the internal service
        IP or kubernetes.default.svc. No other usage is guaranteed across distributions
        of Kubernetes clusters.
    creationTimestamp: "2025-02-10T06:55:41Z"
    name: kube-root-ca.crt
    namespace: kubesphere-monitoring-system
    resourceVersion: "1020684"
    uid: 8e83de28-8068-48fa-a035-71fe0fbad08b
- apiVersion: v1
  data:
    template: |
      {{ define "nm.default.message" }}{{ if ne (len .Status) 0 }}[{{ .Status | translate }}] {{ end }}{{ . | message }}{{ end }}
      {{ define "nm.default.message.cn" }}{{ if ne (len .Status) 0 }}[{{ .Status | translate }}] {{ end }}{{ .MessageCN }}{{ end }}

      {{ define "nm.default.subject" }}{{ if eq (len .Alerts) 1 }}{{ range .Alerts }}{{ template "nm.default.message" . }}{{ end }}{{ else }}{{ .Alerts | len }} {{ if ne (len .Status) 0 }}{{ .Status }} {{ end }}alerts{{ if gt (len .GroupLabels.SortedPairs) 1 }} for {{ range .GroupLabels.SortedPairs }}{{ .Name | translate }}={{ .Value }} {{ end }}{{ end }}{{ end }}{{ end }}

      {{ define "nm.default.text" }}{{ range .Alerts }}{{ template "nm.default.message" . }}
      {{ range .Labels.SortedPairs }}  {{ .Name | translate }}: {{ .Value }}
      {{ end }}
      {{ end }}{{- end }}

      {{ define "nm.default.markdown" }}{{ range .Alerts }}### {{ template "nm.default.message" . }}
      {{ range .Labels.SortedPairs }}- {{ .Name | translate }}: {{ .Value }}
      {{ end }}
      {{ end }}{{- end }}

      {{ define "nm.feishu.post" }}
      en_us:
        content:
          {{ range .Alerts }}
          - - tag: text
              text: "{{ template "nm.default.message" . }}"{{ range .Labels.SortedPairs }}
          - - tag: text
              text: '  {{ .Name | translate }}: {{ .Value }}'
          {{- end }}
          - - tag: text
              text: ' '
          {{- end }}
      {{- end }}

      {{ define "nm.feishu.text" }}{{ range .Alerts }}{{ template "nm.default.message" . }}{{ "\n" }}{{ range .Labels.SortedPairs }}  {{ .Name | translate }}: {{ .Value }}{{ "\n" }}{{ end }}{{ "\n" }}{{ end }}{{- end }}

      {{ define "nm.default.html" }}
        <html xmlns="http://www.w3.org/1999/xhtml" xmlns="http://www.w3.org/1999/xhtml" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;">
        <body itemscope="" itemtype="http://schema.org/EmailMessage" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; -webkit-font-smoothing: antialiased; -webkit-text-size-adjust: none; height: 100%; line-height: 1.6em; width: 100% !important; background-color: #f6f6f6; margin: 0; padding: 0;" bgcolor="#f6f6f6">
        <table style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; width: 100%; background-color: #f6f6f6; margin: 0;" bgcolor="#f6f6f6">
          <tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;">
            <td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0;" valign="top"></td>
            <td width="600" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; display: block !important; max-width: 100% !important; clear: both !important; width: 100% !important; margin: 0 auto; padding: 0;" valign="top">
              <div style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; max-width: 100%; display: block; margin: 0 auto; padding: 0;">
                <table width="100%" cellpadding="0" cellspacing="0" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; border-radius: 3px; background-color: #fff; margin: 0; border: 1px solid #e9e9e9;" bgcolor="#fff">
                  <tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;">
                    <td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 10px;" valign="top">
                      <table width="100%" cellpadding="0" cellspacing="0" style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;">
                        {{ range .Alerts }}
                          <tr style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;">
                            <td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0; padding: 0 0 20px;" valign="top">
                              <strong style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;">{{ template "nm.default.message" . }}</strong><br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" />
                              {{ range .Labels.SortedPairs }}&nbsp;&nbsp;{{ .Name | translate }}: {{ .Value }}<br style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; margin: 0;" />{{ end }}
                            </td>
                          </tr>
                        {{ end }}
                      </table>
                    </td>
                  </tr>
                </table>
              </div>
            </td>
            <td style="font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; box-sizing: border-box; font-size: 14px; vertical-align: top; margin: 0;" valign="top"></td>
          </tr>
        </table>
        </body>
        </html>
      {{ end }}
  kind: ConfigMap
  metadata:
    annotations:
      meta.helm.sh/release-name: notification-manager
      meta.helm.sh/release-namespace: kubesphere-monitoring-system
    creationTimestamp: "2025-02-10T07:02:49Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: notification-manager-template
    namespace: kubesphere-monitoring-system
    resourceVersion: "1023455"
    uid: 1f74213a-e865-4024-8ff9-332b39b3da0c
- apiVersion: v1
  data:
    kubesphere-monitoring-system-alertmanager-main-rules-7b077299-6ff1-40e8-b567-c070daf9648b.yaml: |
      {}
    kubesphere-monitoring-system-kube-state-metrics-rules-ec4da5a5-bd30-4965-8e8e-3c203e339cda.yaml: |
      {}
    kubesphere-monitoring-system-node-exporter-rules-d3bc61e6-7e39-4f60-903f-dab1136b82a4.yaml: |
      {}
    kubesphere-monitoring-system-prometheus-k8s-prometheus-rules-03454703-a373-4d9b-86a6-837e0fc0a3f4.yaml: |
      groups:
      - name: prometheus.rules
        rules:
        - expr: |
            sum by(cluster) (up{job="prometheus-k8s",namespace="kubesphere-monitoring-system"} == 1)
          record: prometheus:up:sum
        - expr: |
            sum(rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"} [5m])) by (job, pod, cluster)
          record: prometheus:prometheus_tsdb_head_samples_appended:sum_rate
    kubesphere-monitoring-system-prometheus-k8s-rules-e95cfff0-cbc3-4683-abeb-875e73e3fe9a.yaml: |
      groups:
      - name: k8s.rules
        rules:
        - expr: |
            sum (irate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!=""}[5m]) * on(namespace, cluster) group_left(workspace) kube_namespace_labels{job="kube-state-metrics"}) by (namespace, workspace, cluster)
            or on(namespace, workspace, cluster) max by(namespace, workspace, cluster) (kube_namespace_labels * 0)
          record: namespace:container_cpu_usage_seconds_total:sum_rate
        - expr: |
            sum(container_memory_usage_bytes{job="kubelet", image!="", container!=""} * on(namespace, cluster) group_left(workspace) kube_namespace_labels{job="kube-state-metrics"}) by (namespace, workspace, cluster)
            or on(namespace, workspace, cluster) max by(namespace, workspace, cluster) (kube_namespace_labels * 0)
          record: namespace:container_memory_usage_bytes:sum
        - expr: |
            sum(container_memory_working_set_bytes{job="kubelet", image!="", container!=""} * on(namespace, cluster) group_left(workspace) kube_namespace_labels{job="kube-state-metrics"}) by (namespace, workspace, cluster)
            or on(namespace, workspace, cluster) max by(namespace, workspace, cluster) (kube_namespace_labels * 0)
          record: namespace:container_memory_usage_bytes_wo_cache:sum
        - expr: |
            sum by (namespace, label_name, cluster) (
                sum(kube_pod_container_resource_requests{resource="memory", job="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service, cluster) group_left(phase) (kube_pod_status_phase{phase=~"Pending|Running"} == 1)) by (namespace, pod, cluster)
              * on (namespace, pod, cluster)
                group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
            )
          record: namespace_memory:kube_pod_container_resource_requests:sum
        - expr: |
            sum by (namespace, label_name, cluster) (
                sum(kube_pod_container_resource_requests{resource="cpu", job="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service, cluster) group_left(phase) (kube_pod_status_phase{phase=~"Pending|Running"} == 1)) by (namespace, pod, cluster)
              * on (namespace, pod, cluster)
                group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
            )
          record: namespace_cpu:kube_pod_container_resource_requests:sum
      - name: node.rules
        rules:
        - expr: |
            sum (node_cpu_seconds_total{job="node-exporter", mode=~"user|nice|system|iowait|irq|softirq"}) by (cpu, instance, job, namespace, pod, cluster)
          record: node_cpu_used_seconds_total
        - expr: |
            max by (namespace,pod,node,owner_name,owner_kind,qos, cluster)(kube_pod_info{job="kube-state-metrics"}
              * on (namespace, pod, cluster) group_left(owner_kind, owner_name) kube_pod_owner{job="kube-state-metrics"}
              * on (namespace,pod, cluster) group_left(qos) max by (namespace,pod,qos, cluster)
                ((label_replace(container_memory_working_set_bytes{job="kubelet", container="",pod!="",id=~".*(burstable|besteffort).*"},"qos","$1","id",".*(burstable|besteffort).*")
                or label_replace(container_memory_working_set_bytes{job="kubelet", container="",pod!="",id!~".*(burstable|besteffort).*"},"qos","guaranteed","id",".*")) > bool 0))
          record: 'qos_owner_node:kube_pod_info:'
        - expr: |
            max(kube_pod_info{job="kube-state-metrics"} * on(node, cluster) group_left(role) kube_node_role{job="kube-state-metrics", role="master"} or on(pod, namespace, cluster) kube_pod_info{job="kube-state-metrics"}) by (node, namespace, host_ip, role, pod, cluster)
          record: 'node_namespace_pod:kube_pod_info:'
        - expr: |
            count by (node, host_ip, role, cluster) (sum by (node, cpu, host_ip, role, cluster) (
              node_cpu_seconds_total{job="node-exporter"}
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:
            ))
          record: node:node_num_cpu:sum
        - expr: |
            avg by(cluster) (irate(node_cpu_used_seconds_total{job="node-exporter"}[5m]))
          record: :node_cpu_utilisation:avg1m
        - expr: |
            avg by (node, host_ip, role, cluster) (
              irate(node_cpu_used_seconds_total{job="node-exporter"}[5m])
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:)
          record: node:node_cpu_utilisation:avg1m
        - expr: |
            1 -
            sum by(cluster) (node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"} + node_memory_SReclaimable_bytes{job="node-exporter"})
            /
            sum by(cluster) (node_memory_MemTotal_bytes{job="node-exporter"})
          record: ':node_memory_utilisation:'
        - expr: |
            sum by (node, host_ip, role, cluster) (
              (node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"} + node_memory_Buffers_bytes{job="node-exporter"} + node_memory_SReclaimable_bytes{job="node-exporter"})
              * on (namespace, pod, cluster) group_left(node, host_ip, role)
                node_namespace_pod:kube_pod_info:
            )
          record: node:node_memory_bytes_available:sum
        - expr: |
            sum by (node, host_ip, role, cluster) (
              node_memory_MemTotal_bytes{job="node-exporter"}
              * on (namespace, pod, cluster) group_left(node, host_ip, role)
                node_namespace_pod:kube_pod_info:
            )
          record: node:node_memory_bytes_total:sum
        - expr: |
            1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
          record: 'node:node_memory_utilisation:'
        - expr: |
            sum by (node, host_ip, role, cluster) (
              irate(node_disk_reads_completed_total{job="node-exporter"}[5m])
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:
            )
          record: node:data_volume_iops_reads:sum
        - expr: |
            sum by (node, host_ip, role, cluster) (
              irate(node_disk_writes_completed_total{job="node-exporter"}[5m])
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:
            )
          record: node:data_volume_iops_writes:sum
        - expr: |
            sum by (node, host_ip, role, cluster) (
              irate(node_disk_read_bytes_total{job="node-exporter"}[5m])
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:
            )
          record: node:data_volume_throughput_bytes_read:sum
        - expr: |
            sum by (node, host_ip, role, cluster) (
              irate(node_disk_written_bytes_total{job="node-exporter"}[5m])
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:
            )
          record: node:data_volume_throughput_bytes_written:sum
        - expr: |
            sum by(cluster) (irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[5m])) +
            sum by(cluster) (irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[5m]))
          record: :node_net_utilisation:sum_irate
        - expr: |
            sum by (node, host_ip, role, cluster) (
              (irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[5m]) +
              irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[5m]))
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:
            )
          record: node:node_net_utilisation:sum_irate
        - expr: |
            sum by (node, host_ip, role, cluster) (
              irate(node_network_transmit_bytes_total{job="node-exporter",device!~"veth.+"}[5m])
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:
            )
          record: node:node_net_bytes_transmitted:sum_irate
        - expr: |
            sum by (node, host_ip, role, cluster) (
              irate(node_network_receive_bytes_total{job="node-exporter",device!~"veth.+"}[5m])
            * on (namespace, pod, cluster) group_left(node, host_ip, role)
              node_namespace_pod:kube_pod_info:
            )
          record: node:node_net_bytes_received:sum_irate
        - expr: |
            sum by(node, host_ip, role, cluster) (sum(max(node_filesystem_files{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"}) by (device, pod, namespace, cluster)) by (pod, namespace, cluster) * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:)
          record: 'node:node_inodes_total:'
        - expr: |
            sum by(node, host_ip, role, cluster) (sum(max(node_filesystem_files_free{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"}) by (device, pod, namespace, cluster)) by (pod, namespace, cluster) * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:)
          record: 'node:node_inodes_free:'
        - expr: |
            sum by (node, host_ip, role, cluster) (node_load1{job="node-exporter"} * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:) / node:node_num_cpu:sum
          record: node:load1:ratio
        - expr: |
            sum by (node, host_ip, role, cluster) (node_load5{job="node-exporter"} * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:) / node:node_num_cpu:sum
          record: node:load5:ratio
        - expr: |
            sum by (node, host_ip, role, cluster) (node_load15{job="node-exporter"} * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:) / node:node_num_cpu:sum
          record: node:load15:ratio
        - expr: |
            sum by (node, host_ip, role, cluster) ((kube_pod_status_scheduled{job="kube-state-metrics", condition="true"} > 0)  * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:)
          record: node:pod_count:sum
        - expr: |
            (sum(kube_node_status_capacity{resource="pods", job="kube-state-metrics"}) by (node, cluster) * on(node, cluster) group_left(host_ip, role) max by(node, host_ip, role, cluster) (node_namespace_pod:kube_pod_info:{node!="",host_ip!=""}))
          record: node:pod_capacity:sum
        - expr: |
            node:pod_running:count / node:pod_capacity:sum
          record: node:pod_utilization:ratio
        - expr: |
            count(node_namespace_pod:kube_pod_info: unless on (pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown|Succeeded"} > 0)) by (node, host_ip, role, cluster)
          record: node:pod_running:count
        - expr: |
            count(node_namespace_pod:kube_pod_info: unless on (pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Failed|Pending|Unknown|Running"} > 0)) by (node, host_ip, role, cluster)
          record: node:pod_succeeded:count
        - expr: |
            count(node_namespace_pod:kube_pod_info:{node!="",host_ip!=""} unless on (pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics", phase="Succeeded"}>0) unless on (pod, namespace, cluster) ((kube_pod_status_ready{job="kube-state-metrics", condition="true"}>0) and on (pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics", phase="Running"}>0)) unless on (pod, namespace, cluster) kube_pod_container_status_waiting_reason{job="kube-state-metrics", reason="ContainerCreating"}>0) by (node, host_ip, role, cluster)
          record: node:pod_abnormal:count
        - expr: |
            node:pod_abnormal:count / count(node_namespace_pod:kube_pod_info:{node!="",host_ip!=""} unless on (pod, namespace, cluster) kube_pod_status_phase{job="kube-state-metrics", phase="Succeeded"}>0) by (node, host_ip, role, cluster)
          record: node:pod_abnormal:ratio
        - expr: |
            sum(max(node_filesystem_avail_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"} * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:) by (device, node, host_ip, role, cluster)) by (node, host_ip, role, cluster)
          record: 'node:disk_space_available:'
        - expr: |
            1- sum(max(node_filesystem_avail_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"} * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:) by (device, node, host_ip, role, cluster)) by (node, host_ip, role, cluster) / sum(max(node_filesystem_size_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"} * on (namespace, pod, cluster) group_left(node, host_ip, role) node_namespace_pod:kube_pod_info:) by (device, node, host_ip, role, cluster)) by (node, host_ip, role, cluster)
          record: node:disk_space_utilization:ratio
        - expr: |
            (1 - (node:node_inodes_free: / node:node_inodes_total:))
          record: node:disk_inode_utilization:ratio
      - name: cluster.rules
        rules:
        - expr: |
            count by(cluster) (kube_pod_info{job="kube-state-metrics"} unless on (pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics", phase="Succeeded"}>0) unless on (pod, namespace, cluster) ((kube_pod_status_ready{job="kube-state-metrics", condition="true"}>0) and on (pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics", phase="Running"}>0)) unless on (pod, namespace, cluster) kube_pod_container_status_waiting_reason{job="kube-state-metrics", reason="ContainerCreating"}>0)
          record: cluster:pod_abnormal:sum
        - expr: |
            sum by(cluster) ((kube_pod_status_scheduled{job="kube-state-metrics", condition="true"} > 0)  * on (namespace, pod, cluster) group_left(node) (sum by (node, namespace, pod, cluster) (kube_pod_info)))
          record: cluster:pod:sum
        - expr: |
            cluster:pod_abnormal:sum / sum by(cluster) (kube_pod_status_phase{job="kube-state-metrics", phase!="Succeeded"})
          record: cluster:pod_abnormal:ratio
        - expr: |
            count by(cluster) (kube_pod_info{job="kube-state-metrics"} and on (pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics", phase="Running"}>0))
          record: cluster:pod_running:count
        - expr: |
            cluster:pod_running:count / sum by(cluster) (kube_node_status_capacity{resource="pods", job="kube-state-metrics"})
          record: cluster:pod_utilization:ratio
        - expr: |
            1 - sum by(cluster) (max(node_filesystem_avail_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"}) by (device, instance, cluster)) / sum by(cluster) (max(node_filesystem_size_bytes{device=~"/dev/.*", device!~"/dev/loop\\d+", job="node-exporter"}) by (device, instance, cluster))
          record: cluster:disk_utilization:ratio
        - expr: |
            1 - sum by(cluster) (node:node_inodes_free:) / sum by(cluster) (node:node_inodes_total:)
          record: cluster:disk_inode_utilization:ratio
        - expr: |
            sum by(cluster) (kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"})
          record: cluster:node_offline:sum
        - expr: |
            sum by(cluster) (kube_node_status_condition{job="kube-state-metrics", condition="Ready", status=~"unknown|false"}) / sum by(cluster) (kube_node_status_condition{job="kube-state-metrics", condition="Ready"})
          record: cluster:node_offline:ratio
      - name: namespace.rules
        rules:
        - expr: |
            (count by(namespace, cluster) (kube_pod_info{job="kube-state-metrics"} unless on(pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics",phase="Succeeded"} > 0) unless on(pod, namespace, cluster) ((kube_pod_status_ready{condition="true",job="kube-state-metrics"} > 0) and on(pod, namespace, cluster) (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} > 0)) unless on(pod, namespace, cluster) kube_pod_container_status_waiting_reason{job="kube-state-metrics",reason="ContainerCreating"} > 0) or on(namespace, cluster) (group by(namespace, cluster) (kube_pod_info{job="kube-state-metrics"}) * 0)) * on(namespace, cluster) group_left(workspace) (kube_namespace_labels{job="kube-state-metrics"}) > 0
          record: namespace:pod_abnormal:count
        - expr: |
            namespace:pod_abnormal:count / (sum(kube_pod_status_phase{job="kube-state-metrics", phase!="Succeeded", namespace!=""}) by (namespace, cluster) * on (namespace, cluster) group_left(workspace)(kube_namespace_labels{job="kube-state-metrics"}))
          record: namespace:pod_abnormal:ratio
        - expr: |
            max(kube_resourcequota{job="kube-state-metrics", type="used"}) by (resource, namespace, cluster) / min(kube_resourcequota{job="kube-state-metrics", type="hard"}) by (resource, namespace, cluster) *  on (namespace, cluster) group_left(workspace) (kube_namespace_labels{job="kube-state-metrics"})
          record: namespace:resourcequota_used:ratio
        - expr: |
            sum (label_replace(label_join(sum(irate(container_cpu_usage_seconds_total{job="kubelet", pod!="", image!=""}[5m])) by (namespace, pod, cluster) * on (pod, namespace, cluster) group_left(owner_kind,owner_name) label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind, cluster)
          record: namespace:workload_cpu_usage:sum
        - expr: |
            sum (label_replace(label_join(sum(container_memory_usage_bytes{job="kubelet", pod!="", image!=""}) by (namespace, pod, cluster) * on (pod, namespace, cluster) group_left(owner_kind,owner_name) label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind, cluster)
          record: namespace:workload_memory_usage:sum
        - expr: |
            sum (label_replace(label_join(sum(container_memory_working_set_bytes{job="kubelet", pod!="", image!=""}) by (namespace, pod, cluster) * on (pod, namespace, cluster) group_left(owner_kind,owner_name) label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind, cluster)
          record: namespace:workload_memory_usage_wo_cache:sum
        - expr: |
            sum (label_replace(label_join(sum(irate(container_network_transmit_bytes_total{pod!="", interface!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)", job="kubelet"}[5m])) by (namespace, pod, cluster) * on (pod, namespace, cluster) group_left(owner_kind,owner_name) label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind, cluster)
          record: namespace:workload_net_bytes_transmitted:sum_irate
        - expr: |
            sum (label_replace(label_join(sum(container_network_transmit_bytes_total{pod!="", interface!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)", job="kubelet"}) by (namespace, pod, cluster) * on (pod, namespace, cluster) group_left(owner_kind,owner_name) label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind, cluster)
          record: namespace:workload_net_bytes_transmitted:sum
        - expr: |
            sum (label_replace(label_join(sum(irate(container_network_receive_bytes_total{pod!="", interface!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)", job="kubelet"}[5m])) by (namespace, pod, cluster) * on (pod, namespace, cluster) group_left(owner_kind,owner_name) label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind, cluster)
          record: namespace:workload_net_bytes_received:sum_irate
        - expr: |
            sum (label_replace(label_join(sum(container_network_receive_bytes_total{pod!="", interface!~"^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|lo.*)", job="kubelet"}) by (namespace, pod, cluster) * on (pod, namespace, cluster) group_left(owner_kind,owner_name) label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind", "Deployment", "owner_kind", "ReplicaSet"), "owner_kind", "Pod", "owner_kind", "<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"), "workload",":","owner_kind","owner_name"), "workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, owner_kind, cluster)
          record: namespace:workload_net_bytes_received:sum
        - expr: |
            label_replace(label_replace(sum(kube_deployment_status_replicas_unavailable{job="kube-state-metrics"}) by (deployment, namespace, cluster) / sum(kube_deployment_spec_replicas{job="kube-state-metrics"}) by (deployment, namespace, cluster) * on (namespace, cluster) group_left(workspace)(kube_namespace_labels{job="kube-state-metrics"}), "workload","Deployment:$1", "deployment", "(.*)"), "owner_kind","Deployment", "", "")
          record: namespace:deployment_unavailable_replicas:ratio
        - expr: |
            label_replace(label_replace(sum(kube_daemonset_status_number_unavailable{job="kube-state-metrics"}) by (daemonset, namespace, cluster) / sum(kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}) by (daemonset, namespace, cluster) * on (namespace, cluster) group_left(workspace)(kube_namespace_labels{job="kube-state-metrics"}) , "workload","DaemonSet:$1", "daemonset", "(.*)"), "owner_kind","DaemonSet", "", "")
          record: namespace:daemonset_unavailable_replicas:ratio
        - expr: |
            label_replace(label_replace((1 - sum(kube_statefulset_status_replicas_current{job="kube-state-metrics"}) by (statefulset, namespace, cluster) / sum(kube_statefulset_replicas{job="kube-state-metrics"}) by (statefulset, namespace, cluster)) * on (namespace, cluster) group_left(workspace)(kube_namespace_labels{job="kube-state-metrics"}) , "workload","StatefulSet:$1", "statefulset", "(.*)"), "owner_kind","StatefulSet", "", "")
          record: namespace:statefulset_unavailable_replicas:ratio
        - expr: |
            sum(kube_pod_container_resource_requests * on (pod, namespace, cluster) group_left(owner_kind,owner_name) label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind","Deployment","owner_kind","ReplicaSet"),"owner_kind","Pod","owner_kind","<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)")) by (namespace, owner_kind, pod, resource, cluster)* on(namespace, cluster) group_left(workspace)kube_namespace_labels{job="kube-state-metrics"}
          record: namespace:kube_pod_resource_request:sum
        - expr: |
            sum(label_replace(label_join(kube_pod_container_resource_requests * on (pod, namespace, cluster) group_left(owner_kind,owner_name)label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind","Deployment","owner_kind","ReplicaSet"),"owner_kind","Pod","owner_kind","<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"),"workload",":","owner_kind","owner_name"),"workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, resource, cluster)* on(namespace, cluster) group_left(workspace) kube_namespace_labels{job="kube-state-metrics"}
          record: namespace:kube_workload_resource_request:sum
        - expr: |
            sum(label_replace(label_join(kube_pod_spec_volumes_persistentvolumeclaims_info * on (pod, namespace, cluster) group_left(owner_kind,owner_name)label_replace(label_join(label_replace(label_replace(kube_pod_owner{job="kube-state-metrics"},"owner_kind","Deployment","owner_kind","ReplicaSet"),"owner_kind","Pod","owner_kind","<none>"),"tmp",":","owner_name","pod"),"owner_name","$1","tmp","<none>:(.*)"),"workload",":","owner_kind","owner_name"),"workload","$1","workload","(Deployment:.+)-(.+)")) by (namespace, workload, pod, persistentvolumeclaim, cluster)* on(namespace, pod, cluster) group_left(node) kube_pod_info{job="kube-state-metrics"}* on (node, persistentvolumeclaim, namespace, cluster) group_left kubelet_volume_stats_capacity_bytes * on(namespace, cluster) group_left(workspace) kube_namespace_labels{job="kube-state-metrics"}
          record: namespace:pvc_bytes_total:sum
      - name: apiserver.rules
        rules:
        - expr: |
            sum by(cluster) (up{job="apiserver"} == 1)
          record: apiserver:up:sum
        - expr: |
            sum by(cluster) (irate(apiserver_request_total{job="apiserver"}[5m]))
          record: apiserver:apiserver_request_total:sum_irate
        - expr: |
            sum(irate(apiserver_request_total{job="apiserver"}[5m])) by (verb, cluster)
          record: apiserver:apiserver_request_total:sum_verb_irate
        - expr: |
            sum by(cluster) (irate(apiserver_request_duration_seconds_sum{job="apiserver",subresource!="log", verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) / sum by(cluster) (irate(apiserver_request_duration_seconds_count{job="apiserver", subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m]))
          record: apiserver:apiserver_request_duration:avg
        - expr: |
            sum(irate(apiserver_request_duration_seconds_sum{job="apiserver",subresource!="log", verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) by (verb, cluster) / sum(irate(apiserver_request_duration_seconds_count{job="apiserver", subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) by (verb, cluster)
          record: apiserver:apiserver_request_duration:avg_by_verb
      - name: scheduler.rules
        rules:
        - expr: |
            sum by(cluster) (up{job="kube-scheduler"} == 1)
          record: scheduler:up:sum
        - expr: |
            sum(scheduler_schedule_attempts_total{job="kube-scheduler"}) by (result, cluster)
          record: scheduler:scheduler_schedule_attempts:sum
        - expr: |
            sum(rate(scheduler_schedule_attempts_total{job="kube-scheduler"}[5m])) by (result, cluster)
          record: scheduler:scheduler_schedule_attempts:sum_rate
        - expr: |
            (sum by(cluster) (rate(scheduler_e2e_scheduling_duration_seconds_sum{job="kube-scheduler"}[1h]))  / sum by(cluster) (rate(scheduler_e2e_scheduling_duration_seconds_count{job="kube-scheduler"}[1h])))
          record: scheduler:scheduler_e2e_scheduling_duration:avg
      - name: scheduler_histogram.rules
        rules:
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[1h])) by (le, cluster) )
          labels:
            quantile: "0.99"
          record: scheduler:scheduler_e2e_scheduling_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[1h])) by (le, cluster) )
          labels:
            quantile: "0.9"
          record: scheduler:scheduler_e2e_scheduling_duration:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[1h])) by (le, cluster) )
          labels:
            quantile: "0.5"
          record: scheduler:scheduler_e2e_scheduling_duration:histogram_quantile
      - name: controller_manager.rules
        rules:
        - expr: |
            sum by(cluster) (up{job="kube-controller-manager"} == 1)
          record: controller_manager:up:sum
      - name: coredns.rules
        rules:
        - expr: |
            sum by(cluster) (up{job="coredns"} == 1)
          record: coredns:up:sum
      - name: kube-apiserver-burnrate.rules
        rules:
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1d]))
                -
                (
                  (
                    sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="1"}[1d]))
                    or
                    vector(0)
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="5"}[1d]))
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="30"}[1d]))
                )
              )
              +
              # errors
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d]))
          labels:
            verb: read
          record: apiserver_request:burnrate1d
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[1h]))
                -
                (
                  (
                    sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="1"}[1h]))
                    or
                    vector(0)
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="5"}[1h]))
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="30"}[1h]))
                )
              )
              +
              # errors
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1h]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1h]))
          labels:
            verb: read
          record: apiserver_request:burnrate1h
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[2h]))
                -
                (
                  (
                    sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="1"}[2h]))
                    or
                    vector(0)
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="5"}[2h]))
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="30"}[2h]))
                )
              )
              +
              # errors
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[2h]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[2h]))
          labels:
            verb: read
          record: apiserver_request:burnrate2h
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[30m]))
                -
                (
                  (
                    sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="1"}[30m]))
                    or
                    vector(0)
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="5"}[30m]))
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="30"}[30m]))
                )
              )
              +
              # errors
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[30m]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))
          labels:
            verb: read
          record: apiserver_request:burnrate30m
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[3d]))
                -
                (
                  (
                    sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="1"}[3d]))
                    or
                    vector(0)
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="5"}[3d]))
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="30"}[3d]))
                )
              )
              +
              # errors
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[3d]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[3d]))
          labels:
            verb: read
          record: apiserver_request:burnrate3d
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[5m]))
                -
                (
                  (
                    sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="1"}[5m]))
                    or
                    vector(0)
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="5"}[5m]))
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="30"}[5m]))
                )
              )
              +
              # errors
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[5m]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
          labels:
            verb: read
          record: apiserver_request:burnrate5m
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET"}[6h]))
                -
                (
                  (
                    sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope=~"resource|",le="1"}[6h]))
                    or
                    vector(0)
                  )
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="namespace",le="5"}[6h]))
                  +
                  sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="30"}[6h]))
                )
              )
              +
              # errors
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[6h]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))
          labels:
            verb: read
          record: apiserver_request:burnrate6h
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
                -
                sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[1d]))
              )
              +
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1d]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
          labels:
            verb: write
          record: apiserver_request:burnrate1d
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
                -
                sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[1h]))
              )
              +
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
          labels:
            verb: write
          record: apiserver_request:burnrate1h
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
                -
                sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[2h]))
              )
              +
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[2h]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
          labels:
            verb: write
          record: apiserver_request:burnrate2h
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
                -
                sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[30m]))
              )
              +
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[30m]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
          labels:
            verb: write
          record: apiserver_request:burnrate30m
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
                -
                sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[3d]))
              )
              +
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[3d]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
          labels:
            verb: write
          record: apiserver_request:burnrate3d
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
                -
                sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[5m]))
              )
              +
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[5m]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
          labels:
            verb: write
          record: apiserver_request:burnrate5m
        - expr: |
            (
              (
                # too slow
                sum by (cluster) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
                -
                sum by (cluster) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",le="1"}[6h]))
              )
              +
              sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[6h]))
            )
            /
            sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
          labels:
            verb: write
          record: apiserver_request:burnrate6h
      - name: kube-apiserver-histogram.rules
        rules:
        - expr: |
            histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET"}[5m]))) > 0
          labels:
            quantile: "0.99"
            verb: read
          record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))) > 0
          labels:
            quantile: "0.99"
            verb: write
          record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - interval: 3m
        name: kube-apiserver-availability.rules
        rules:
        - expr: |
            avg_over_time(code_verb:apiserver_request_total:increase1h[30d]) * 24 * 30
          record: code_verb:apiserver_request_total:increase30d
        - expr: |
            sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})
          labels:
            verb: read
          record: code:apiserver_request_total:increase30d
        - expr: |
            sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
          labels:
            verb: write
          record: code:apiserver_request_total:increase30d
        - expr: |
            sum by (cluster, verb, scope) (increase(apiserver_request_duration_seconds_count[1h]))
          record: cluster_verb_scope:apiserver_request_duration_seconds_count:increase1h
        - expr: |
            sum by (cluster, verb, scope) (avg_over_time(cluster_verb_scope:apiserver_request_duration_seconds_count:increase1h[30d]) * 24 * 30)
          record: cluster_verb_scope:apiserver_request_duration_seconds_count:increase30d
        - expr: |
            sum by (cluster, verb, scope, le) (increase(apiserver_request_duration_seconds_bucket[1h]))
          record: cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase1h
        - expr: |
            sum by (cluster, verb, scope, le) (avg_over_time(cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase1h[30d]) * 24 * 30)
          record: cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d
        - expr: |
            1 - (
              (
                # write too slow
                sum by (cluster) (cluster_verb_scope:apiserver_request_duration_seconds_count:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
                -
                sum by (cluster) (cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d{verb=~"POST|PUT|PATCH|DELETE",le="1"})
              ) +
              (
                # read too slow
                sum by (cluster) (cluster_verb_scope:apiserver_request_duration_seconds_count:increase30d{verb=~"LIST|GET"})
                -
                (
                  (
                    sum by (cluster) (cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope=~"resource|",le="1"})
                    or
                    vector(0)
                  )
                  +
                  sum by (cluster) (cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="namespace",le="5"})
                  +
                  sum by (cluster) (cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="cluster",le="30"})
                )
              ) +
              # errors
              sum by (cluster) (code:apiserver_request_total:increase30d{code=~"5.."} or vector(0))
            )
            /
            sum by (cluster) (code:apiserver_request_total:increase30d)
          labels:
            verb: all
          record: apiserver_request:availability30d
        - expr: |
            1 - (
              sum by (cluster) (cluster_verb_scope:apiserver_request_duration_seconds_count:increase30d{verb=~"LIST|GET"})
              -
              (
                # too slow
                (
                  sum by (cluster) (cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope=~"resource|",le="1"})
                  or
                  vector(0)
                )
                +
                sum by (cluster) (cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="namespace",le="5"})
                +
                sum by (cluster) (cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="cluster",le="30"})
              )
              +
              # errors
              sum by (cluster) (code:apiserver_request_total:increase30d{verb="read",code=~"5.."} or vector(0))
            )
            /
            sum by (cluster) (code:apiserver_request_total:increase30d{verb="read"})
          labels:
            verb: read
          record: apiserver_request:availability30d
        - expr: |
            1 - (
              (
                # too slow
                sum by (cluster) (cluster_verb_scope:apiserver_request_duration_seconds_count:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
                -
                sum by (cluster) (cluster_verb_scope_le:apiserver_request_duration_seconds_bucket:increase30d{verb=~"POST|PUT|PATCH|DELETE",le="1"})
              )
              +
              # errors
              sum by (cluster) (code:apiserver_request_total:increase30d{verb="write",code=~"5.."} or vector(0))
            )
            /
            sum by (cluster) (code:apiserver_request_total:increase30d{verb="write"})
          labels:
            verb: write
          record: apiserver_request:availability30d
        - expr: |
            sum by (cluster,code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
          labels:
            verb: read
          record: code_resource:apiserver_request_total:rate5m
        - expr: |
            sum by (cluster,code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
          labels:
            verb: write
          record: code_resource:apiserver_request_total:rate5m
        - expr: |
            sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"2.."}[1h]))
          record: code_verb:apiserver_request_total:increase1h
        - expr: |
            sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"3.."}[1h]))
          record: code_verb:apiserver_request_total:increase1h
        - expr: |
            sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"4.."}[1h]))
          record: code_verb:apiserver_request_total:increase1h
        - expr: |
            sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
          record: code_verb:apiserver_request_total:increase1h
      - name: kubelet.rules
        rules:
        - expr: |
            histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le, cluster) * on(instance, cluster) group_left(node) kubelet_node_name{job="kubelet"})
          labels:
            quantile: "0.99"
          record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le, cluster) * on(instance, cluster) group_left(node) kubelet_node_name{job="kubelet"})
          labels:
            quantile: "0.9"
          record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) by (instance, le, cluster) * on(instance, cluster) group_left(node) kubelet_node_name{job="kubelet"})
          labels:
            quantile: "0.5"
          record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    kubesphere-monitoring-system-prometheus-operator-rules-d351954b-69c4-4d7e-a290-cbc4d9dd7311.yaml: |
      {}
    kubesphere-monitoring-system-thanos-ruler-kubesphere-rules-0713f1f9-a4d5-4139-8ad8-8c2c2d7a8eda.yaml: |
      {}
  kind: ConfigMap
  metadata:
    creationTimestamp: "2025-03-04T08:46:14Z"
    labels:
      managed-by: prometheus-operator
      prometheus-name: k8s
    name: prometheus-k8s-rulefiles-0
    namespace: kubesphere-monitoring-system
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: Prometheus
      name: k8s
      uid: 4b040f98-bedd-48d7-8312-46382d0fa134
    resourceVersion: "11877369"
    uid: 475815cc-2266-4a6b-a0e8-660c2183c4a6
- apiVersion: v1
  data:
    kubesphere-monitoring-system-alertrules-cl-0.yaml: |
      groups:
      - interval: 1m
        name: pod-down-alert
        rules:
        - alert: PodDownAlert
          annotations:
            description: A pod in the POC namespace has entered a critical state.
            message: |-
              - Namespace: POC
              - Condition: One or more pods are in a "Failed" or "Down" state.
              - Time Triggered: {{ .StartsAt }}

              Please investigate immediately to ensure system stability.
            pod_name: '{{ $labels.pod }}'
            summary: "\U0001F6A8 Pod Down Alert \U0001F6A8"
          expr: |
            sum(rate(kube_pod_status_phase{namespace="poc", phase=~"Failed|CrashLoopBackOff|Error|ImagePullBackOff|Pending"}[1m])) > 0
          for: 10s
          labels:
            alerttype: metric
            rule_group: pod-down-alert
            rule_id: ac6273c6-8aa4-4025-8835-7235f844e0b5
            rule_level: cluster
            rule_type: custom
            severity: critical
    kubesphere-monitoring-system-alertrules-gl-0.yaml: |
      groups:
      - name: alertmanager-rules
        rules:
        - alert: AlertmanagerFailedReload
          annotations:
            description: Configuration has failed to load for {{ $labels.namespace }}/{{
              $labels.pod}}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/alertmanager/alertmanagerfailedreload
            summary: Reloading an Alertmanager configuration has failed.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="kubesphere-monitoring-system"}[5m]) == 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: alertmanager-rules
            rule_id: eab2d5ca2d8460850ad30dc27d726063
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: AlertmanagerMembersInconsistent
          annotations:
            description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only
              found {{ $value }} members of the {{$labels.job}} cluster.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/alertmanager/alertmanagermembersinconsistent
            summary: A member of an Alertmanager cluster has not found all other cluster
              members.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
              max_over_time(alertmanager_cluster_members{job="alertmanager-main",namespace="kubesphere-monitoring-system"}[5m])
            < on (cluster,namespace,service) group_left
              count by (cluster,namespace,service) (max_over_time(alertmanager_cluster_members{job="alertmanager-main",namespace="kubesphere-monitoring-system"}[5m]))
          for: 15m
          labels:
            alerttype: metric
            rule_group: alertmanager-rules
            rule_id: 7d74f5a7464e75655f61fd44c9b9382c
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: AlertmanagerFailedToSendAlerts
          annotations:
            description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to
              send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration
              }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/alertmanager/alertmanagerfailedtosendalerts
            summary: An Alertmanager instance failed to send notifications.
          expr: |
            (
              rate(alertmanager_notifications_failed_total{job="alertmanager-main",namespace="kubesphere-monitoring-system"}[5m])
            /
              rate(alertmanager_notifications_total{job="alertmanager-main",namespace="kubesphere-monitoring-system"}[5m])
            )
            > 0.01
          for: 5m
          labels:
            alerttype: metric
            rule_group: alertmanager-rules
            rule_id: 98e8d890268d7edde59e8557ca084020
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: AlertmanagerClusterFailedToSendAlerts
          annotations:
            description: The minimum notification failure rate to {{ $labels.integration
              }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage
              }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts
            summary: All Alertmanager instances in a cluster failed to send notifications
              to a critical integration.
          expr: |
            min by (cluster,namespace,service, integration) (
              rate(alertmanager_notifications_failed_total{job="alertmanager-main",namespace="kubesphere-monitoring-system", integration=~`.*`}[5m])
            /
              rate(alertmanager_notifications_total{job="alertmanager-main",namespace="kubesphere-monitoring-system", integration=~`.*`}[5m])
            )
            > 0.01
          for: 5m
          labels:
            alerttype: metric
            rule_group: alertmanager-rules
            rule_id: b376895c08b94ab96a01743d2f32436e
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: AlertmanagerClusterFailedToSendAlerts
          annotations:
            description: The minimum notification failure rate to {{ $labels.integration
              }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage
              }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts
            summary: All Alertmanager instances in a cluster failed to send notifications
              to a non-critical integration.
          expr: |
            min by (cluster,namespace,service, integration) (
              rate(alertmanager_notifications_failed_total{job="alertmanager-main",namespace="kubesphere-monitoring-system", integration!~`.*`}[5m])
            /
              rate(alertmanager_notifications_total{job="alertmanager-main",namespace="kubesphere-monitoring-system", integration!~`.*`}[5m])
            )
            > 0.01
          for: 5m
          labels:
            alerttype: metric
            rule_group: alertmanager-rules
            rule_id: 3c8b88da259e058ca7413009a2de8b7a
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: AlertmanagerConfigInconsistent
          annotations:
            description: Alertmanager instances within the {{$labels.job}} cluster have
              different configurations.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/alertmanager/alertmanagerconfiginconsistent
            summary: Alertmanager instances within the same cluster have different configurations.
          expr: |
            count by (cluster,namespace,service) (
              count_values by (cluster,namespace,service) ("config_hash", alertmanager_config_hash{job="alertmanager-main",namespace="kubesphere-monitoring-system"})
            )
            != 1
          for: 20m
          labels:
            alerttype: metric
            rule_group: alertmanager-rules
            rule_id: c66c3dab57e66549118aa3bfba0ca421
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: AlertmanagerClusterDown
          annotations:
            description: '{{ $value | humanizePercentage }} of Alertmanager instances within
              the {{$labels.job}} cluster have been up for less than half of the last 5m.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/alertmanager/alertmanagerclusterdown
            summary: Half or more of the Alertmanager instances within the same cluster
              are down.
          expr: |
            (
              count by (cluster,namespace,service) (
                avg_over_time(up{job="alertmanager-main",namespace="kubesphere-monitoring-system"}[5m]) < 0.5
              )
            /
              count by (cluster,namespace,service) (
                up{job="alertmanager-main",namespace="kubesphere-monitoring-system"}
              )
            )
            >= 0.5
          for: 5m
          labels:
            alerttype: metric
            rule_group: alertmanager-rules
            rule_id: 032af165d93d54d29ff29eae1951f15b
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: AlertmanagerClusterCrashlooping
          annotations:
            description: '{{ $value | humanizePercentage }} of Alertmanager instances within
              the {{$labels.job}} cluster have restarted at least 5 times in the last 10m.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/alertmanager/alertmanagerclustercrashlooping
            summary: Half or more of the Alertmanager instances within the same cluster
              are crashlooping.
          expr: |
            (
              count by (cluster,namespace,service) (
                changes(process_start_time_seconds{job="alertmanager-main",namespace="kubesphere-monitoring-system"}[10m]) > 4
              )
            /
              count by (cluster,namespace,service) (
                up{job="alertmanager-main",namespace="kubesphere-monitoring-system"}
              )
            )
            >= 0.5
          for: 5m
          labels:
            alerttype: metric
            rule_group: alertmanager-rules
            rule_id: a2ec3e19a125d4b13112a37ca0069ad8
            rule_level: global
            rule_type: custom
            severity: critical
      - name: config-reloaders
        rules:
        - alert: ConfigReloaderSidecarErrors
          annotations:
            description: |-
              Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
              As a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus-operator/configreloadersidecarerrors
            summary: config-reloader sidecar has not had a successful reload for 10m
          expr: |
            max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: config-reloaders
            rule_id: 39693c93edd5a4ce8b4f23d7de5024b3
            rule_level: global
            rule_type: custom
            severity: warning
      - name: ks-apiserver
        rules:
        - alert: ksApiSlow
          annotations:
            message: 99th percentile of requests is {{ $value }}s on ks-apiserver instance
              {{ $labels.instance }} for {{ $labels.verb }} {{ $labels.resource }}.{{ $labels.group
              }}/{{ $labels.version }}
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubesphere/ksapislow
            summary: ks-apiserver requests are slow.
          expr: |
            histogram_quantile(0.99, sum by(instance,group,resource,verb,version,le,cluster) (rate(ks_server_request_duration_seconds_bucket{group!="terminal.kubesphere.io", job="ks-apiserver"}[5m]))) > 5
          for: 10m
          labels:
            alerttype: metric
            rule_group: ks-apiserver
            rule_id: 3f5d58fdbd68199d4ff515bfd09a2f87
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: ksApiserverDown
          annotations:
            description: ksApiserver has disappeared from Prometheus target discovery.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubesphere/ksapiserverdown
            summary: Target disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="ks-apiserver"} == 1)
          for: 15m
          labels:
            alerttype: metric
            rule_group: ks-apiserver
            rule_id: 766f549cf2737e1adc0c62e4f5fd0ba6
            rule_level: global
            rule_type: custom
            severity: critical
      - name: ks-controller-manager
        rules:
        - alert: ksControllerManagerDown
          annotations:
            description: ksControllerManager has disappeared from Prometheus target discovery.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubesphere/kscontrollermanagerdown
            summary: Target disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="ks-controller-manager"} == 1)
          for: 15m
          labels:
            alerttype: metric
            rule_group: ks-controller-manager
            rule_id: 6367d0bcc67cf87eab8fb4e72e2285be
            rule_level: global
            rule_type: custom
            severity: critical
      - name: kube-apiserver-slos
        rules:
        - alert: KubeAPIErrorBudgetBurn
          annotations:
            description: The API server is burning too much error budget.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeapierrorbudgetburn
            summary: The API server is burning too much error budget.
          expr: |
            sum by(cluster) (apiserver_request:burnrate1h) > (14.40 * 0.01000)
            and
            sum by(cluster) (apiserver_request:burnrate5m) > (14.40 * 0.01000)
          for: 2m
          labels:
            alerttype: metric
            long: 1h
            rule_group: kube-apiserver-slos
            rule_id: 496dc38f79520016b294241cc24bad59
            rule_level: global
            rule_type: custom
            severity: critical
            short: 5m
        - alert: KubeAPIErrorBudgetBurn
          annotations:
            description: The API server is burning too much error budget.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeapierrorbudgetburn
            summary: The API server is burning too much error budget.
          expr: |
            sum by(cluster) (apiserver_request:burnrate6h) > (6.00 * 0.01000)
            and
            sum by(cluster) (apiserver_request:burnrate30m) > (6.00 * 0.01000)
          for: 15m
          labels:
            alerttype: metric
            long: 6h
            rule_group: kube-apiserver-slos
            rule_id: 496dc38f79520016b294241cc24bad59
            rule_level: global
            rule_type: custom
            severity: critical
            short: 30m
        - alert: KubeAPIErrorBudgetBurn
          annotations:
            description: The API server is burning too much error budget.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeapierrorbudgetburn
            summary: The API server is burning too much error budget.
          expr: |
            sum by(cluster) (apiserver_request:burnrate1d) > (3.00 * 0.01000)
            and
            sum by(cluster) (apiserver_request:burnrate2h) > (3.00 * 0.01000)
          for: 1h
          labels:
            alerttype: metric
            long: 1d
            rule_group: kube-apiserver-slos
            rule_id: d91e8041b76c5861056ae6a61dabd8d7
            rule_level: global
            rule_type: custom
            severity: warning
            short: 2h
        - alert: KubeAPIErrorBudgetBurn
          annotations:
            description: The API server is burning too much error budget.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeapierrorbudgetburn
            summary: The API server is burning too much error budget.
          expr: |
            sum by(cluster) (apiserver_request:burnrate3d) > (1.00 * 0.01000)
            and
            sum by(cluster) (apiserver_request:burnrate6h) > (1.00 * 0.01000)
          for: 3h
          labels:
            alerttype: metric
            long: 3d
            rule_group: kube-apiserver-slos
            rule_id: d91e8041b76c5861056ae6a61dabd8d7
            rule_level: global
            rule_type: custom
            severity: warning
            short: 6h
      - name: kube-state-metrics
        rules:
        - alert: KubeStateMetricsListErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in
              list operations. This is likely causing it to not be able to expose metrics
              about Kubernetes objects correctly or at all.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kube-state-metrics/kubestatemetricslisterrors
            summary: kube-state-metrics is experiencing errors in list operations.
          expr: |
            (sum by(cluster,namespace,service) (rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m]))
              /
            sum by(cluster,namespace,service) (rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])))
            > 0.01
          for: 15m
          labels:
            alerttype: metric
            rule_group: kube-state-metrics
            rule_id: 0e7897c5cfbca8933deedd7475be2e6f
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: KubeStateMetricsWatchErrors
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in
              watch operations. This is likely causing it to not be able to expose metrics
              about Kubernetes objects correctly or at all.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kube-state-metrics/kubestatemetricswatcherrors
            summary: kube-state-metrics is experiencing errors in watch operations.
          expr: |
            (sum by(cluster,namespace,service) (rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m]))
              /
            sum by(cluster,namespace,service) (rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])))
            > 0.01
          for: 15m
          labels:
            alerttype: metric
            rule_group: kube-state-metrics
            rule_id: 152ccb43660a19968a830ae2050f1855
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: KubeStateMetricsShardingMismatch
          annotations:
            description: kube-state-metrics pods are running with different --total-shards
              configuration, some Kubernetes objects may be exposed multiple times or not
              exposed at all.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch
            summary: kube-state-metrics sharding is misconfigured.
          expr: |
            stdvar by(cluster,namespace,service) (kube_state_metrics_total_shards{job="kube-state-metrics"}) != 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kube-state-metrics
            rule_id: 1e8f579922412497e9b3d29c412b31fe
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: KubeStateMetricsShardsMissing
          annotations:
            description: kube-state-metrics shards are missing, some Kubernetes objects
              are not being exposed.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kube-state-metrics/kubestatemetricsshardsmissing
            summary: kube-state-metrics shards are missing.
          expr: |
            2^max by(cluster,namespace,service) (kube_state_metrics_total_shards{job="kube-state-metrics"}) - 1
              -
            sum by(cluster,namespace,service) ( 2 ^ max by (shard_ordinal,cluster,namespace,service) (kube_state_metrics_shard_ordinal{job="kube-state-metrics"}) )
            != 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kube-state-metrics
            rule_id: d859094c0a5ac6357cab4c776ccd5093
            rule_level: global
            rule_type: custom
            severity: critical
      - name: kubernetes-apps
        rules:
        - alert: KubePodCrashLooping
          annotations:
            description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
              }}) is in waiting state (reason: "CrashLoopBackOff").'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubepodcrashlooping
            summary: Pod is crash looping.
          expr: |
            max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", job="kube-state-metrics"}[5m]) >= 1
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 543ffcdc373563bca2c24096ee0d1c49
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubePodNotReady
          annotations:
            description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
              state for longer than 15 minutes.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubepodnotready
            summary: Pod has been in a non-ready state for more than 15 minutes.
          expr: |
            sum by (namespace, pod, cluster) (
              max by(namespace, pod, cluster) (
                kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}
              ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
                1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
              )
            ) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: aacdce877d5cc082474b61ef16f9c0de
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeDeploymentGenerationMismatch
          annotations:
            description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
              }} does not match, this indicates that the Deployment has failed but has not
              been rolled back.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubedeploymentgenerationmismatch
            summary: Deployment generation mismatch due to possible roll-back
          expr: |
            kube_deployment_status_observed_generation{job="kube-state-metrics"}
              !=
            kube_deployment_metadata_generation{job="kube-state-metrics"}
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 501fdb906a41f1e20422c466944d56c8
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeDeploymentReplicasMismatch
          annotations:
            description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has
              not matched the expected number of replicas for longer than 15 minutes.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubedeploymentreplicasmismatch
            summary: Deployment has not matched the expected number of replicas.
          expr: |
            (
              kube_deployment_spec_replicas{job="kube-state-metrics"}
                >
              kube_deployment_status_replicas_available{job="kube-state-metrics"}
            ) and (
              changes(kube_deployment_status_replicas_updated{job="kube-state-metrics"}[10m])
                ==
              0
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: a13278b0c277d54a7d1a37431874edc9
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeStatefulSetReplicasMismatch
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
              not matched the expected number of replicas for longer than 15 minutes.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubestatefulsetreplicasmismatch
            summary: StatefulSet has not matched the expected number of replicas.
          expr: |
            (
              kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
                !=
              kube_statefulset_status_replicas{job="kube-state-metrics"}
            ) and (
              changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics"}[10m])
                ==
              0
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: fa2619c5995ed34956b97e142d638119
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeStatefulSetGenerationMismatch
          annotations:
            description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
              }} does not match, this indicates that the StatefulSet has failed but has
              not been rolled back.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubestatefulsetgenerationmismatch
            summary: StatefulSet generation mismatch due to possible roll-back
          expr: |
            kube_statefulset_status_observed_generation{job="kube-state-metrics"}
              !=
            kube_statefulset_metadata_generation{job="kube-state-metrics"}
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 5c129ee8ac6f56b846d7e075a9c37a9f
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeStatefulSetUpdateNotRolledOut
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
              has not been rolled out.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
            summary: StatefulSet update has not been rolled out.
          expr: |
            (
              max without (revision) (
                kube_statefulset_status_current_revision{job="kube-state-metrics"}
                  unless
                kube_statefulset_status_update_revision{job="kube-state-metrics"}
              )
                *
              (
                kube_statefulset_replicas{job="kube-state-metrics"}
                  !=
                kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
              )
            )  and (
              changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics"}[5m])
                ==
              0
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: f022b4df5f3bf17c76c2b5d4b312fd76
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeDaemonSetRolloutStuck
          annotations:
            description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not
              finished or progressed for at least 15 minutes.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubedaemonsetrolloutstuck
            summary: DaemonSet rollout is stuck.
          expr: |
            (
              (
                kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              ) or (
                kube_daemonset_status_number_misscheduled{job="kube-state-metrics"}
                 !=
                0
              ) or (
                kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              ) or (
                kube_daemonset_status_number_available{job="kube-state-metrics"}
                 !=
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              )
            ) and (
              changes(kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics"}[5m])
                ==
              0
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 32ff60c86df79f0a167299c46ed123eb
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeContainerWaiting
          annotations:
            description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container
              {{ $labels.container}} has been in waiting state for longer than 1 hour.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubecontainerwaiting
            summary: Pod container waiting longer than 1 hour
          expr: |
            sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job="kube-state-metrics"}) > 0
          for: 1h
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: b614db45c1e2c8ae67a899cce3e1d54c
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeDaemonSetNotScheduled
          annotations:
            description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
              }} are not scheduled.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubedaemonsetnotscheduled
            summary: DaemonSet pods are not scheduled.
          expr: |
            kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
              -
            kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 82f48068fb7f9809e52d09e2100275cc
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeDaemonSetMisScheduled
          annotations:
            description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
              }} are running where they are not supposed to run.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubedaemonsetmisscheduled
            summary: DaemonSet pods are misscheduled.
          expr: |
            kube_daemonset_status_number_misscheduled{job="kube-state-metrics"} > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 9800bd48dbfe64a3d5cd7b3d57ccdc05
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeJobCompletion
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
              than 12 hours to complete.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubejobcompletion
            summary: Job did not complete in time
          expr: |
            kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
          for: 12h
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 0714251ec2bafcc7f79c5e8622e35df6
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeJobFailed
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
              Removing failed job after investigation should clear this alert.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubejobfailed
            summary: Job failed to complete.
          expr: |
            kube_job_failed{job="kube-state-metrics"}  > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 846cc9dbb403019632d41736b828659c
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeHpaReplicasMismatch
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
              has not matched the desired number of replicas for longer than 15 minutes.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubehpareplicasmismatch
            summary: HPA has not matched descired number of replicas.
          expr: |
            (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics"}
              !=
            kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"})
              and
            (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
              >
            kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics"})
              and
            (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
              <
            kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics"})
              and
            changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}[15m]) == 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 27db79265cf0035fc0d163517b3356d8
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeHpaMaxedOut
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
              has been running at max replicas for longer than 15 minutes.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubehpamaxedout
            summary: HPA is running at max replicas
          expr: |
            kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics"}
              ==
            kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics"}
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-apps
            rule_id: 2019ad93816ce4c10d92028e41af4b7f
            rule_level: global
            rule_type: custom
            severity: warning
      - name: kubernetes-resources
        rules:
        - alert: KubeCPUOvercommit
          annotations:
            description: Cluster has overcommitted CPU resource requests for Pods by {{
              $value }} CPU shares and cannot tolerate node failure.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubecpuovercommit
            summary: Cluster has overcommitted CPU resource requests.
          expr: |
            sum by(cluster) (namespace_cpu:kube_pod_container_resource_requests:sum{}) - (sum by(cluster) (kube_node_status_allocatable{resource="cpu"}) - max by(cluster) (kube_node_status_allocatable{resource="cpu"})) > 0
            and
            (sum by(cluster) (kube_node_status_allocatable{resource="cpu"}) - max by(cluster) (kube_node_status_allocatable{resource="cpu"})) > 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: kubernetes-resources
            rule_id: b64aed32564452d2ce55cab27c3e6fd7
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeMemoryOvercommit
          annotations:
            description: Cluster has overcommitted memory resource requests for Pods by
              {{ $value | humanize }} bytes and cannot tolerate node failure.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubememoryovercommit
            summary: Cluster has overcommitted memory resource requests.
          expr: |
            sum by(cluster) (namespace_memory:kube_pod_container_resource_requests:sum{}) - (sum by(cluster) (kube_node_status_allocatable{resource="memory"}) - max by(cluster) (kube_node_status_allocatable{resource="memory"})) > 0
            and
            (sum by(cluster) (kube_node_status_allocatable{resource="memory"}) - max by(cluster) (kube_node_status_allocatable{resource="memory"})) > 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: kubernetes-resources
            rule_id: 47f3ab42838b2beb3bd3f6f48bc3511b
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeCPUQuotaOvercommit
          annotations:
            description: Cluster has overcommitted CPU resource requests for Namespaces.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubecpuquotaovercommit
            summary: Cluster has overcommitted CPU resource requests.
          expr: |
            sum by(cluster) (min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(cpu|requests.cpu)"}))
              /
            sum by(cluster) (kube_node_status_allocatable{resource="cpu", job="kube-state-metrics"})
              > 1.5
          for: 5m
          labels:
            alerttype: metric
            rule_group: kubernetes-resources
            rule_id: 980edb0ab5e444f21b963a919127ff29
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeMemoryQuotaOvercommit
          annotations:
            description: Cluster has overcommitted memory resource requests for Namespaces.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubememoryquotaovercommit
            summary: Cluster has overcommitted memory resource requests.
          expr: |
            sum by(cluster) (min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(memory|requests.memory)"}))
              /
            sum by(cluster) (kube_node_status_allocatable{resource="memory", job="kube-state-metrics"})
              > 1.5
          for: 5m
          labels:
            alerttype: metric
            rule_group: kubernetes-resources
            rule_id: ad587cd6161b8c409783dbb5472c3c44
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeQuotaAlmostFull
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
              }} of its {{ $labels.resource }} quota.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubequotaalmostfull
            summary: Namespace quota is going to be full.
          expr: |
            kube_resourcequota{job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
              > 0.9 < 1
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-resources
            rule_id: 8652aecc8236a9ebd0c511b9150754e4
            rule_level: global
            rule_type: custom
            severity: info
        - alert: KubeQuotaFullyUsed
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
              }} of its {{ $labels.resource }} quota.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubequotafullyused
            summary: Namespace quota is fully used.
          expr: |
            kube_resourcequota{job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
              == 1
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-resources
            rule_id: 55ca94b73491ca9495206fcfc2880a9b
            rule_level: global
            rule_type: custom
            severity: info
        - alert: KubeQuotaExceeded
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
              }} of its {{ $labels.resource }} quota.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubequotaexceeded
            summary: Namespace quota has exceeded the limits.
          expr: |
            kube_resourcequota{job="kube-state-metrics", type="used"}
              / ignoring(instance, job, type)
            (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
              > 1
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-resources
            rule_id: f85a8c5993f29d1c958a472d11a0c4ca
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: CPUThrottlingHigh
          annotations:
            description: '{{ $value | humanizePercentage }} throttling of CPU in namespace
              {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod
              }}.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/cputhrottlinghigh
            summary: Processes experience elevated CPU throttling.
          expr: |
            sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace, cluster)
              /
            sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace, cluster)
              > ( 25 / 100 )
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-resources
            rule_id: fd50dcebe1f425e797e30b8b4114695d
            rule_level: global
            rule_type: custom
            severity: info
      - name: kubernetes-storage
        rules:
        - alert: KubePersistentVolumeFillingUp
          annotations:
            description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage
              }} free.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubepersistentvolumefillingup
            summary: PersistentVolume is filling up.
          expr: |
            (
              kubelet_volume_stats_available_bytes{job="kubelet"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubelet"}
            ) < 0.03
            and
            kubelet_volume_stats_used_bytes{job="kubelet"} > 0
            unless on(namespace, persistentvolumeclaim, cluster)
            kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
            unless on(namespace, persistentvolumeclaim, cluster)
            kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
          for: 1m
          labels:
            alerttype: metric
            rule_group: kubernetes-storage
            rule_id: 1401c9e1b119e95ccde46056e869caf9
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: KubePersistentVolumeFillingUp
          annotations:
            description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
              }} in Namespace {{ $labels.namespace }} is expected to fill up within four
              days. Currently {{ $value | humanizePercentage }} is available.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubepersistentvolumefillingup
            summary: PersistentVolume is filling up.
          expr: |
            (
              kubelet_volume_stats_available_bytes{job="kubelet"}
                /
              kubelet_volume_stats_capacity_bytes{job="kubelet"}
            ) < 0.15
            and
            kubelet_volume_stats_used_bytes{job="kubelet"} > 0
            and
            predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
            unless on(namespace, persistentvolumeclaim, cluster)
            kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
            unless on(namespace, persistentvolumeclaim, cluster)
            kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
          for: 1h
          labels:
            alerttype: metric
            rule_group: kubernetes-storage
            rule_id: f4df94231db4ba454be67e3c7056314c
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubePersistentVolumeErrors
          annotations:
            description: The persistent volume {{ $labels.persistentvolume }} has status
              {{ $labels.phase }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubepersistentvolumeerrors
            summary: PersistentVolume is having issues with provisioning.
          expr: |
            (kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0) * on(cluster, persistentvolume) group_left(pvc_namespace, pvc_name) label_replace(label_replace(label_replace(kube_persistentvolumeclaim_info{job="kube-state-metrics"}, "persistentvolume", "$1", "volumename", "(.*)"), "pvc_namespace", "$1", "namespace", "(.*)"), "pvc_name", "$1", "persistentvolumeclaim", "(.*)")
          for: 5m
          labels:
            alerttype: metric
            rule_group: kubernetes-storage
            rule_id: 10d15b499fb505b2913d458c9f59617a
            rule_level: global
            rule_type: custom
            severity: critical
      - name: kubernetes-system
        rules:
        - alert: KubeVersionMismatch
          annotations:
            description: There are {{ $value }} different semantic versions of Kubernetes
              components running.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeversionmismatch
            summary: Different semantic versions of Kubernetes components running.
          expr: |
            count by(cluster) (count by (git_version, cluster) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"git_version","$1","git_version","(v[0-9]*.[0-9]*).*"))) > 1
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system
            rule_id: c72c8ce8f9cb198349d17001489b37d9
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeClientErrors
          annotations:
            description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
              }}' is experiencing {{ $value | humanizePercentage }} errors.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeclienterrors
            summary: Kubernetes API server client is experiencing errors.
          expr: |
            (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job, namespace, cluster)
              /
            sum(rate(rest_client_requests_total[5m])) by (instance, job, namespace, cluster))
            > 0.01
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system
            rule_id: 0d831b24d7375c8df8841adf0fb355ab
            rule_level: global
            rule_type: custom
            severity: warning
      - name: kubernetes-system-apiserver
        rules:
        - alert: KubeClientCertificateExpiration
          annotations:
            description: A client certificate used to authenticate to kubernetes apiserver
              is expiring in less than 7.0 days.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeclientcertificateexpiration
            summary: Client certificate is about to expire.
          expr: |
            apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and on(job, cluster) histogram_quantile(0.01, sum by (job, le, cluster) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
          labels:
            alerttype: metric
            rule_group: kubernetes-system-apiserver
            rule_id: b7059bc6d7945d82d32793d9b6467b9c
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeClientCertificateExpiration
          annotations:
            description: A client certificate used to authenticate to kubernetes apiserver
              is expiring in less than 24.0 hours.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeclientcertificateexpiration
            summary: Client certificate is about to expire.
          expr: |
            apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and on(job, cluster) histogram_quantile(0.01, sum by (job, le, cluster) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
          labels:
            alerttype: metric
            rule_group: kubernetes-system-apiserver
            rule_id: 34feba1ca44cd76ab80901f4ccfd842e
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: KubeAggregatedAPIErrors
          annotations:
            description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace
              }} has reported errors. It has appeared unavailable {{ $value | humanize }}
              times averaged over the past 10m.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeaggregatedapierrors
            summary: Kubernetes aggregated API has reported errors.
          expr: |
            sum by(name, namespace, cluster)(increase(aggregator_unavailable_apiservice_total[10m])) > 4
          labels:
            alerttype: metric
            rule_group: kubernetes-system-apiserver
            rule_id: eb24c1c858d39e40b0c3312441726ad2
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeAggregatedAPIDown
          annotations:
            description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace
              }} has been only {{ $value | humanize }}% available over the last 10m.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeaggregatedapidown
            summary: Kubernetes aggregated API is down.
          expr: |
            (1 - max by(name, namespace, cluster)(avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 < 85
          for: 5m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-apiserver
            rule_id: b5f953a7e412a96c15b54bee94cca57f
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeAPIDown
          annotations:
            description: KubeAPI has disappeared from Prometheus target discovery.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeapidown
            summary: Target disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="apiserver"} == 1)
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-apiserver
            rule_id: 2e09d689aa5d601c52f61ed4e7d877d1
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: KubeAPITerminatedRequests
          annotations:
            description: The kubernetes apiserver has terminated {{ $value | humanizePercentage
              }} of its incoming requests.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeapiterminatedrequests
            summary: The kubernetes apiserver has terminated {{ $value | humanizePercentage
              }} of its incoming requests.
          expr: |
            sum by(cluster) (rate(apiserver_request_terminations_total{job="apiserver"}[10m]))  / (  sum by(cluster) (rate(apiserver_request_total{job="apiserver"}[10m])) + sum by(cluster) (rate(apiserver_request_terminations_total{job="apiserver"}[10m])) ) > 0.20
          for: 5m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-apiserver
            rule_id: ce44dac277794fb8727ea6f6d0696226
            rule_level: global
            rule_type: custom
            severity: warning
      - name: kubernetes-system-controller-manager
        rules:
        - alert: KubeControllerManagerDown
          annotations:
            description: KubeControllerManager has disappeared from Prometheus target discovery.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubecontrollermanagerdown
            summary: Target disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="kube-controller-manager"} == 1)
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-controller-manager
            rule_id: 1d578c1a1146b22e3f92f954e3bb27d5
            rule_level: global
            rule_type: custom
            severity: critical
      - name: kubernetes-system-kubelet
        rules:
        - alert: KubeNodeNotReady
          annotations:
            description: '{{ $labels.node }} has been unready for more than 15 minutes.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubenodenotready
            summary: Node is not ready.
          expr: |
            kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 3f0ce7b052e4d0badbba3f2b425829e5
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeNodeUnreachable
          annotations:
            description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubenodeunreachable
            summary: Node is unreachable.
          expr: |
            (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 065ecdb97eafa89aefe3b67dbb3c260a
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeletTooManyPods
          annotations:
            description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
              }} of its Pod capacity.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubelettoomanypods
            summary: Kubelet is running at capacity.
          expr: |
            count by(node, cluster) (
              (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
            )
            /
            max by(node, cluster) (
              kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
            ) > 0.95
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 638892a2ea1cc7edb408f5b17e5ba448
            rule_level: global
            rule_type: custom
            severity: info
        - alert: KubeNodeReadinessFlapping
          annotations:
            description: The readiness status of node {{ $labels.node }} has changed {{
              $value }} times in the last 15 minutes.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubenodereadinessflapping
            summary: Node readiness status is flapping.
          expr: |
            sum(changes(kube_node_status_condition{status="true",condition="Ready"}[15m])) by (node, cluster) > 2
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 58758efc62806bf92e1db4073d9d8ab1
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeletPlegDurationHigh
          annotations:
            description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile
              duration of {{ $value }} seconds on node {{ $labels.node }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletplegdurationhigh
            summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
          expr: |
            node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
          for: 5m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 7cff1995f51770daab46159cf915b273
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeletPodStartUpLatencyHigh
          annotations:
            description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds
              on node {{ $labels.node }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletpodstartuplatencyhigh
            summary: Kubelet Pod startup latency is too high.
          expr: |
            histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet"}[5m])) by (instance, le, cluster)) * on(instance, cluster) group_left(node) kubelet_node_name{job="kubelet"} > 60
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 09a4a2750246f41618b39120d40db7fd
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeletClientCertificateExpiration
          annotations:
            description: Client certificate for Kubelet on node {{ $labels.node }} expires
              in {{ $value | humanizeDuration }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletclientcertificateexpiration
            summary: Kubelet client certificate is about to expire.
          expr: |
            kubelet_certificate_manager_client_ttl_seconds < 604800
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: a53c8aca6a1d12ce044ccf9dded58be8
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeletClientCertificateExpiration
          annotations:
            description: Client certificate for Kubelet on node {{ $labels.node }} expires
              in {{ $value | humanizeDuration }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletclientcertificateexpiration
            summary: Kubelet client certificate is about to expire.
          expr: |
            kubelet_certificate_manager_client_ttl_seconds < 86400
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 7368753ddd4bba718dd45aebddd60ff7
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: KubeletServerCertificateExpiration
          annotations:
            description: Server certificate for Kubelet on node {{ $labels.node }} expires
              in {{ $value | humanizeDuration }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletservercertificateexpiration
            summary: Kubelet server certificate is about to expire.
          expr: |
            kubelet_certificate_manager_server_ttl_seconds < 604800
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: b38ab98563ca8cdae3017b38a6db923c
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeletServerCertificateExpiration
          annotations:
            description: Server certificate for Kubelet on node {{ $labels.node }} expires
              in {{ $value | humanizeDuration }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletservercertificateexpiration
            summary: Kubelet server certificate is about to expire.
          expr: |
            kubelet_certificate_manager_server_ttl_seconds < 86400
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 3ebb385fe27a27ac35c7b0a8baea9d33
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: KubeletClientCertificateRenewalErrors
          annotations:
            description: Kubelet on node {{ $labels.node }} has failed to renew its client
              certificate ({{ $value | humanize }} errors in the last 5 minutes).
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletclientcertificaterenewalerrors
            summary: Kubelet has failed to renew its client certificate.
          expr: |
            increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 38d17ac4b585ec992ac16f65ff2a36f7
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeletServerCertificateRenewalErrors
          annotations:
            description: Kubelet on node {{ $labels.node }} has failed to renew its server
              certificate ({{ $value | humanize }} errors in the last 5 minutes).
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletservercertificaterenewalerrors
            summary: Kubelet has failed to renew its server certificate.
          expr: |
            increase(kubelet_server_expiration_renew_errors[5m]) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: f38d371051332f545895768d556d0c53
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: KubeletDown
          annotations:
            description: Kubelet has disappeared from Prometheus target discovery.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeletdown
            summary: Target disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="kubelet"} == 1)
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-kubelet
            rule_id: 33a2afc614b1735fed4085980457aed5
            rule_level: global
            rule_type: custom
            severity: critical
      - name: kubernetes-system-scheduler
        rules:
        - alert: KubeSchedulerDown
          annotations:
            description: KubeScheduler has disappeared from Prometheus target discovery.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/kubernetes/kubeschedulerdown
            summary: Target disappeared from Prometheus target discovery.
          expr: |
            absent(up{job="kube-scheduler"} == 1)
          for: 15m
          labels:
            alerttype: metric
            rule_group: kubernetes-system-scheduler
            rule_id: f2c13f81fb2d40c8d34305bf4d60162f
            rule_level: global
            rule_type: custom
            severity: critical
      - name: node-exporter
        rules:
        - alert: NodeFilesystemSpaceFillingUp
          annotations:
            description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
              only {{ printf "%.2f" $value }}% available space left and is filling up.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefilesystemspacefillingup
            summary: Filesystem is predicted to run out of space within the next 24 hours.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 15
            and
              predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 1edc003491066bcfc10f5aecbd4e2c7f
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeFilesystemSpaceFillingUp
          annotations:
            description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
              only {{ printf "%.2f" $value }}% available space left and is filling up fast.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefilesystemspacefillingup
            summary: Filesystem is predicted to run out of space within the next 4 hours.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 10
            and
              predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 269f46476b7e978eb1cce0dc4a13fddf
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: NodeFilesystemAlmostOutOfSpace
          annotations:
            description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
              only {{ printf "%.2f" $value }}% available space left.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefilesystemalmostoutofspace
            summary: Filesystem has less than 5% space left.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 5
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 30m
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 9d746c60b086a39e5acdd80fb8f18599
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeFilesystemAlmostOutOfSpace
          annotations:
            description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
              only {{ printf "%.2f" $value }}% available space left.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefilesystemalmostoutofspace
            summary: Filesystem has less than 3% space left.
          expr: |
            (
              node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 3
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 30m
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 6f6f322dadd7b1c094d621727981ef93
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: NodeFilesystemFilesFillingUp
          annotations:
            description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
              only {{ printf "%.2f" $value }}% available inodes left and is filling up.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefilesystemfilesfillingup
            summary: Filesystem is predicted to run out of inodes within the next 24 hours.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
            and
              predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: d244aba8ed103d3a664ef5773648b623
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeFilesystemFilesFillingUp
          annotations:
            description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
              only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefilesystemfilesfillingup
            summary: Filesystem is predicted to run out of inodes within the next 4 hours.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
            and
              predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: a2d5196f25a1572eac3344eb782f1fb9
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: NodeFilesystemAlmostOutOfFiles
          annotations:
            description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
              only {{ printf "%.2f" $value }}% available inodes left.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefilesystemalmostoutoffiles
            summary: Filesystem has less than 5% inodes left.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 5
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 704d345a95aa7cd4db217d7740256d65
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeFilesystemAlmostOutOfFiles
          annotations:
            description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
              only {{ printf "%.2f" $value }}% available inodes left.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefilesystemalmostoutoffiles
            summary: Filesystem has less than 3% inodes left.
          expr: |
            (
              node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
            and
              node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
            )
          for: 1h
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 7c20b6519e0548c6835f119c72e5c602
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: NodeNetworkReceiveErrs
          annotations:
            description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
              {{ printf "%.0f" $value }} receive errors in the last two minutes.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodenetworkreceiveerrs
            summary: Network interface is reporting many receive errors.
          expr: |
            rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
          for: 1h
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 82d8fa238e1526b7238f3dbb4f8d5d9c
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeNetworkTransmitErrs
          annotations:
            description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
              {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodenetworktransmiterrs
            summary: Network interface is reporting many transmit errors.
          expr: |
            rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
          for: 1h
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: b28d986367cb39577fc77c18a70e5eb1
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeHighNumberConntrackEntriesUsed
          annotations:
            description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodehighnumberconntrackentriesused
            summary: Number of conntrack are getting close to the limit.
          expr: |
            (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: d83943b90de240b9c6b22076380d0449
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeTextFileCollectorScrapeError
          annotations:
            description: Node Exporter text file collector failed to scrape.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodetextfilecollectorscrapeerror
            summary: Node Exporter text file collector failed to scrape.
          expr: |
            node_textfile_scrape_error{job="node-exporter"} == 1
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 96df8aa76998ea01acd652db4ca59708
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeClockSkewDetected
          annotations:
            description: Clock on {{ $labels.instance }} is out of sync by more than 300s.
              Ensure NTP is configured correctly on this host.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodeclockskewdetected
            summary: Clock skew detected.
          expr: |
            (
              node_timex_offset_seconds > 0.05
            and
              deriv(node_timex_offset_seconds[5m]) >= 0
            )
            or
            (
              node_timex_offset_seconds < -0.05
            and
              deriv(node_timex_offset_seconds[5m]) <= 0
            )
          for: 10m
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: b6d779ce1e7ac1c836f0f73ba53719e8
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeClockNotSynchronising
          annotations:
            description: Clock on {{ $labels.instance }} is not synchronising. Ensure NTP
              is configured on this host.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodeclocknotsynchronising
            summary: Clock not synchronising.
          expr: |
            min_over_time(node_timex_sync_status[5m]) == 0
            and
            node_timex_maxerror_seconds >= 16
          for: 10m
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: 45ef5f64189fb6e5c7612446a5ea7325
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeRAIDDegraded
          annotations:
            description: RAID array '{{ $labels.device }}' on {{ $labels.instance }} is
              in degraded state due to one or more disks failures. Number of spare drives
              is insufficient to fix issue automatically.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/noderaiddegraded
            summary: RAID Array is degraded
          expr: |
            node_md_disks_required - ignoring (state) (node_md_disks{state="active"}) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: d57c855e9c8398ad7397281275269ca1
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: NodeRAIDDiskFailure
          annotations:
            description: At least one device in RAID array on {{ $labels.instance }} failed.
              Array '{{ $labels.device }}' needs attention and possibly a disk swap.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/noderaiddiskfailure
            summary: Failed device in RAID array
          expr: |
            node_md_disks{state="failed"} > 0
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: d206195006307821a803109973d71609
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeFileDescriptorLimit
          annotations:
            description: File descriptors limit at {{ $labels.instance }} is currently at
              {{ printf "%.2f" $value }}%.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefiledescriptorlimit
            summary: Kernel is predicted to exhaust file descriptors limit soon.
          expr: |
            (
              node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: c870eace8c067b5c439e65f6f04af084
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: NodeFileDescriptorLimit
          annotations:
            description: File descriptors limit at {{ $labels.instance }} is currently at
              {{ printf "%.2f" $value }}%.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/node/nodefiledescriptorlimit
            summary: Kernel is predicted to exhaust file descriptors limit soon.
          expr: |
            (
              node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: node-exporter
            rule_id: c975f50b606f686233d8a1f7b5459c05
            rule_level: global
            rule_type: custom
            severity: critical
      - name: prometheus
        rules:
        - alert: PrometheusBadConfig
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
              reload its configuration.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusbadconfig
            summary: Failed Prometheus configuration reload.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(prometheus_config_last_reload_successful{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) == 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: ca2cc23c549b60414fb07574a3203436
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: PrometheusNotificationQueueRunningFull
          annotations:
            description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}}
              is running full.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusnotificationqueuerunningfull
            summary: Prometheus alert notification queue predicted to run full in less than
              30m.
          expr: |
            # Without min_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            (
              predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m], 60 * 30)
            >
              min_over_time(prometheus_notifications_queue_capacity{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m])
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 5c0326759757bc274cf00c6eccd96e0f
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
          annotations:
            description: '{{ printf "%.1f" $value }}% errors while sending alerts from Prometheus
              {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers
            summary: Prometheus has encountered more than 1% errors sending alerts to a
              specific Alertmanager.
          expr: |
            (
              rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m])
            /
              rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m])
            )
            * 100
            > 1
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 0b56e353f19760eb44c1a8e35818ccb5
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusNotConnectedToAlertmanagers
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected
              to any Alertmanagers.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusnotconnectedtoalertmanagers
            summary: Prometheus is not connected to any Alertmanagers.
          expr: |
            sum without(rule_group) (prometheus_rule_group_rules{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}) > 0
            and
            max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) < 1
          for: 10m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: a3664f7af82c07e87d1e172aeca6cc81
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusTSDBReloadsFailing
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value
              | humanize}} reload failures over the last 3h.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheustsdbreloadsfailing
            summary: Prometheus has issues reloading blocks from disk.
          expr: |
            increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[3h]) > 0
          for: 4h
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: e3abfbb67cc1d5b2a85408a8bcf6fe55
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusTSDBCompactionsFailing
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value
              | humanize}} compaction failures over the last 3h.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheustsdbcompactionsfailing
            summary: Prometheus has issues compacting blocks.
          expr: |
            increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[3h]) > 0
          for: 4h
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 312d3ffc78d42e8a54e1dae518683d3f
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusNotIngestingSamples
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting
              samples.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusnotingestingsamples
            summary: Prometheus is not ingesting samples.
          expr: |
            (
              rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) <= 0
            and
              (
                sum without(scrape_job) (prometheus_target_metadata_cache_entries{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}) > 0
              or
                sum without(rule_group) (prometheus_rule_group_rules{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}) > 0
              )
            )
          for: 10m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 1b85ea812e6abe3c2438dff685b3b932
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusDuplicateTimestamps
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{
              printf "%.4g" $value  }} samples/s with different values but duplicated timestamp.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusduplicatetimestamps
            summary: Prometheus is dropping samples with duplicate timestamps.
          expr: |
            rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) > 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 156c2663944cc3ea76d9db8bfd35d00f
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusOutOfOrderTimestamps
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{
              printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusoutofordertimestamps
            summary: Prometheus drops samples with out-of-order timestamps.
          expr: |
            rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) > 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: b8375ea714bfc625123983c6ddd341c5
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusRemoteStorageFailures
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send
              {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{
              $labels.url }}
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusremotestoragefailures
            summary: Prometheus fails to send samples to remote storage.
          expr: |
            (
              (rate(prometheus_remote_storage_failed_samples_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]))
            /
              (
                (rate(prometheus_remote_storage_failed_samples_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]))
              +
                (rate(prometheus_remote_storage_succeeded_samples_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) or rate(prometheus_remote_storage_samples_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]))
              )
            )
            * 100
            > 1
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 224f85b6fa54731368e40c9dd805c688
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: PrometheusRemoteWriteBehind
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is
              {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url
              }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusremotewritebehind
            summary: Prometheus remote write is behind.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            (
              max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m])
            - ignoring(remote_name, url) group_right
              max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m])
            )
            > 120
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 4abb1d42f6d664e6eca70cf40feb6327
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: PrometheusRemoteWriteDesiredShards
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired
              shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{
              $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="prometheus-k8s",namespace="kubesphere-monitoring-system"}`
              $labels.instance | query | first | value }}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusremotewritedesiredshards
            summary: Prometheus remote write desired shards calculation wants to run more
              than configured max shards.
          expr: |
            # Without max_over_time, failed scrapes could create false negatives, see
            # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            (
              max_over_time(prometheus_remote_storage_shards_desired{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m])
            >
              max_over_time(prometheus_remote_storage_shards_max{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m])
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 2cb93f24af8b5af92f338a92f662ec16
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusRuleFailures
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
              evaluate {{ printf "%.0f" $value }} rules in the last 5m.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusrulefailures
            summary: Prometheus is failing rule evaluations.
          expr: |
            increase(prometheus_rule_evaluation_failures_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 6ef5814a59ac81ec16d71b7807f1918a
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: PrometheusMissingRuleEvaluations
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{
              printf "%.0f" $value }} rule group evaluations in the last 5m.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheusmissingruleevaluations
            summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
          expr: |
            increase(prometheus_rule_group_iterations_missed_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 3f3e841abb5688c373fd94c1bfe9aea5
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusTargetLimitHit
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{
              printf "%.0f" $value }} targets because the number of targets exceeded the
              configured target_limit.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheustargetlimithit
            summary: Prometheus has dropped targets because some scrape configs have exceeded
              the targets limit.
          expr: |
            increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: b1cc4c194cc080f11011ef496eea6534
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusLabelLimitHit
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{
              printf "%.0f" $value }} targets because some samples exceeded the configured
              label_limit, label_name_length_limit or label_value_length_limit.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheuslabellimithit
            summary: Prometheus has dropped targets because some scrape configs have exceeded
              the labels limit.
          expr: |
            increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[5m]) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 71d75164a3fd47aa3ea80dd411dde197
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusTargetSyncFailure
          annotations:
            description: '{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}}
              have failed to sync because invalid configuration was supplied.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheustargetsyncfailure
            summary: Prometheus has failed to sync targets.
          expr: |
            increase(prometheus_target_sync_failed_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system"}[30m]) > 0
          for: 5m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 09085b0a6cea108ea28189891fdb95bc
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
          annotations:
            description: '{{ printf "%.1f" $value }}% minimum errors while sending alerts
              from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager
            summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
          expr: |
            min without (alertmanager) (
              rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system",alertmanager!~``}[5m])
            /
              rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="kubesphere-monitoring-system",alertmanager!~``}[5m])
            )
            * 100
            > 3
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus
            rule_id: 47edf187b983b2556026a98fa4bb25e3
            rule_level: global
            rule_type: custom
            severity: critical
      - name: prometheus-operator
        rules:
        - alert: PrometheusOperatorListErrors
          annotations:
            description: Errors while performing List operations in controller {{$labels.controller}}
              in {{$labels.namespace}} namespace.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus-operator/prometheusoperatorlisterrors
            summary: Errors while performing list operations in controller.
          expr: |
            (sum by (controller,namespace,cluster) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-operator",namespace="kubesphere-monitoring-system"}[10m])) / sum by (controller,namespace,cluster) (rate(prometheus_operator_list_operations_total{job="prometheus-operator",namespace="kubesphere-monitoring-system"}[10m]))) > 0.4
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus-operator
            rule_id: 6434951db224a2bf20dca8e27951a6c6
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusOperatorWatchErrors
          annotations:
            description: Errors while performing watch operations in controller {{$labels.controller}}
              in {{$labels.namespace}} namespace.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus-operator/prometheusoperatorwatcherrors
            summary: Errors while performing watch operations in controller.
          expr: |
            (sum by (controller,namespace,cluster) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-operator",namespace="kubesphere-monitoring-system"}[10m])) / sum by (controller,namespace,cluster) (rate(prometheus_operator_watch_operations_total{job="prometheus-operator",namespace="kubesphere-monitoring-system"}[10m]))) > 0.4
          for: 15m
          labels:
            alerttype: metric
            rule_group: prometheus-operator
            rule_id: 5bf4e154b275e355b444342e9134505d
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusOperatorSyncFailed
          annotations:
            description: Controller {{ $labels.controller }} in {{ $labels.namespace }}
              namespace fails to reconcile {{ $value }} objects.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus-operator/prometheusoperatorsyncfailed
            summary: Last controller reconciliation failed
          expr: |
            min_over_time(prometheus_operator_syncs{status="failed",job="prometheus-operator",namespace="kubesphere-monitoring-system"}[5m]) > 0
          for: 10m
          labels:
            alerttype: metric
            rule_group: prometheus-operator
            rule_id: fc63b335443718bbf66ce8610762d29f
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusOperatorReconcileErrors
          annotations:
            description: '{{ $value | humanizePercentage }} of reconciling operations failed
              for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.'
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus-operator/prometheusoperatorreconcileerrors
            summary: Errors while reconciling controller.
          expr: |
            (sum by (controller,namespace,cluster) (rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator",namespace="kubesphere-monitoring-system"}[5m]))) / (sum by (controller,namespace,cluster) (rate(prometheus_operator_reconcile_operations_total{job="prometheus-operator",namespace="kubesphere-monitoring-system"}[5m]))) > 0.1
          for: 10m
          labels:
            alerttype: metric
            rule_group: prometheus-operator
            rule_id: c588b3706841edc0a495aafbce7574c4
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusOperatorNodeLookupErrors
          annotations:
            description: Errors while reconciling Prometheus in {{ $labels.namespace }}
              Namespace.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus-operator/prometheusoperatornodelookuperrors
            summary: Errors while reconciling Prometheus.
          expr: |
            rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator",namespace="kubesphere-monitoring-system"}[5m]) > 0.1
          for: 10m
          labels:
            alerttype: metric
            rule_group: prometheus-operator
            rule_id: ae4a4a44956fda827f43df1f76990fe1
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusOperatorNotReady
          annotations:
            description: Prometheus operator in {{ $labels.namespace }} namespace isn't
              ready to reconcile {{ $labels.controller }} resources.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus-operator/prometheusoperatornotready
            summary: Prometheus operator not ready
          expr: |
            min by(controller,namespace,cluster) (max_over_time(prometheus_operator_ready{job="prometheus-operator",namespace="kubesphere-monitoring-system"}[5m]) == 0)
          for: 5m
          labels:
            alerttype: metric
            rule_group: prometheus-operator
            rule_id: 3539a37778aeda49796c61bc8a62e791
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: PrometheusOperatorRejectedResources
          annotations:
            description: Prometheus operator in {{ $labels.namespace }} namespace rejected
              {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource }}
              resources.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/prometheus-operator/prometheusoperatorrejectedresources
            summary: Resources rejected by Prometheus operator
          expr: |
            min_over_time(prometheus_operator_managed_resources{state="rejected",job="prometheus-operator",namespace="kubesphere-monitoring-system"}[5m]) > 0
          for: 5m
          labels:
            alerttype: metric
            rule_group: prometheus-operator
            rule_id: 3357d98377c72df2ffcda6d58cce1d00
            rule_level: global
            rule_type: custom
            severity: warning
      - name: thanos-rule
        rules:
        - alert: ThanosRuleQueueIsDroppingAlerts
          annotations:
            description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
              to queue alerts.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosrulequeueisdroppingalerts
            summary: Thanos Rule is failing to queue alerts.
          expr: |
            sum by (cluster, namespace, job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m])) > 0
          for: 5m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 8a663511d4642025f5a0580fd618d89b
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: ThanosRuleSenderIsFailingAlerts
          annotations:
            description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
              to send alerts to alertmanager.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosrulesenderisfailingalerts
            summary: Thanos Rule is failing to send alerts to alertmanager.
          expr: |
            sum by (cluster, namespace, job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m])) > 0
          for: 5m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 7e994d7a9912689118fe68327d516d74
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: ThanosRuleHighRuleEvaluationFailures
          annotations:
            description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing
              to evaluate rules.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosrulehighruleevaluationfailures
            summary: Thanos Rule is failing to evaluate rules.
          expr: |
            (
              sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluation_failures_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
            /
              sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluations_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
            * 100 > 5
            )
          for: 5m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 1369506100b9d16e1ee4c5369b07eb4d
            rule_level: global
            rule_type: custom
            severity: critical
        - alert: ThanosRuleHighRuleEvaluationWarnings
          annotations:
            description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has high
              number of evaluation warnings.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosrulehighruleevaluationwarnings
            summary: Thanos Rule has high number of evaluation warnings.
          expr: |
            sum by (cluster, namespace, job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m])) > 0
          for: 15m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: e86609a38691ffe545d2a8da6743dfab
            rule_level: global
            rule_type: custom
            severity: info
        - alert: ThanosRuleRuleEvaluationLatencyHigh
          annotations:
            description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has higher
              evaluation latency than interval for {{$labels.rule_group}}.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosruleruleevaluationlatencyhigh
            summary: Thanos Rule has high rule evaluation latency.
          expr: |
            (
              sum by (cluster, namespace, job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
            >
              sum by (cluster, namespace, job, instance, rule_group) (prometheus_rule_group_interval_seconds{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
            )
          for: 5m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 386d20888c0984a7a5640e1627c3fb99
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: ThanosRuleGrpcErrorRate
          annotations:
            description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} is failing
              to handle {{$value | humanize}}% of requests.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosrulegrpcerrorrate
            summary: Thanos Rule is failing to handle grpc requests.
          expr: |
            (
              sum by (cluster, namespace, job, instance) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
            /
              sum by (cluster, namespace, job, instance) (rate(grpc_server_started_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
            * 100 > 5
            )
          for: 5m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 6cefe8c838cd7b1b9fc8ac3a67e7fed6
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: ThanosRuleConfigReloadFailure
          annotations:
            description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has not been
              able to reload its configuration.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosruleconfigreloadfailure
            summary: Thanos Rule has not been able to reload configuration.
          expr: avg by (cluster, namespace, job, instance) (thanos_rule_config_last_reload_successful{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
            != 1
          for: 5m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 71de0adaceee2af75a09beb08a0062a2
            rule_level: global
            rule_type: custom
            severity: info
        - alert: ThanosRuleQueryHighDNSFailures
          annotations:
            description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value
              | humanize}}% of failing DNS queries for query endpoints.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosrulequeryhighdnsfailures
            summary: Thanos Rule is having high number of DNS failures.
          expr: |
            (
              sum by (cluster, namespace, job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
            /
              sum by (cluster, namespace, job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
            * 100 > 1
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 70d976940a6ee03ab86a658abf3a0c08
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: ThanosRuleAlertmanagerHighDNSFailures
          annotations:
            description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has {{$value
              | humanize}}% of failing DNS queries for Alertmanager endpoints.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosrulealertmanagerhighdnsfailures
            summary: Thanos Rule is having high number of DNS failures.
          expr: |
            (
              sum by (cluster, namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
            /
              sum by (cluster, namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m]))
            * 100 > 1
            )
          for: 15m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 8cbe90cf6e5abac46fa60f29b9b3dc12
            rule_level: global
            rule_type: custom
            severity: warning
        - alert: ThanosRuleNoEvaluationFor10Intervals
          annotations:
            description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value
              | humanize}}% rule groups that did not evaluate for at least 10x of their
              expected interval.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosrulenoevaluationfor10intervals
            summary: Thanos Rule has rule groups that did not evaluate for 10 intervals.
          expr: |
            time() -  max by (cluster, namespace, job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
            >
            10 * max by (cluster, namespace, job, instance, group) (prometheus_rule_group_interval_seconds{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"})
          for: 5m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 96e6fa0bcbecd8b9c2720a168bb9ae68
            rule_level: global
            rule_type: custom
            severity: info
        - alert: ThanosNoRuleEvaluations
          annotations:
            description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} did not
              perform any rule evaluations in the past 10 minutes.
            runbook_url: https://alert-runbooks.kubesphere.io/runbooks/thanos/thanosnoruleevaluations
            summary: Thanos Rule did not perform any rule evaluations.
          expr: |
            sum by (cluster, namespace, job, instance) (rate(prometheus_rule_evaluations_total{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}[5m])) <= 0
              and
            sum by (cluster, namespace, job, instance) (thanos_rule_loaded_rules{job="thanos-ruler-kubesphere",namespace="kubesphere-monitoring-system"}) > 0
          for: 5m
          labels:
            alerttype: metric
            rule_group: thanos-rule
            rule_id: 8aa44c144c2a9504e9905d445136dcdb
            rule_level: global
            rule_type: custom
            severity: critical
  kind: ConfigMap
  metadata:
    creationTimestamp: "2025-04-25T07:53:47Z"
    labels:
      managed-by: prometheus-operator
      thanos-ruler-name: kubesphere
    name: thanos-ruler-kubesphere-rulefiles-0
    namespace: kubesphere-monitoring-system
    ownerReferences:
    - apiVersion: monitoring.coreos.com/v1
      blockOwnerDeletion: true
      controller: true
      kind: ThanosRuler
      name: kubesphere
      uid: 8f59f3d8-07d7-4bef-a0a9-b0522abba605
    resourceVersion: "40710335"
    uid: ca5a9cf0-5fd4-4077-b79b-3c45883ea216
- apiVersion: v1
  data:
    zh-cn: "- name: zh-cn\n  dictionary:\n    alert: \"告警\"\n    alerts: \"告警\" \n
      \   firing: \"触发中\"\n    resolved: \"已解决\"\n    alertname: \"告警名称\"\n    alerttype:
      \"告警类型\"\n    alerttime: \"告警时间\"\n    cluster: \"集群\"\n    namespace: \"项目\"\n
      \   severity: \"告警级别\"\n    container: \"容器\"\n    pod: \"容器组\"\n    service:
      \"服务\"\n    deployment: \"部署\"\n    job: \"任务\"\n    daemonset: \"守护进程集\"\n
      \   statefulset: \"有状态副本集\"\n    instance: \"实例\"\n    resource: \"资源\"\n    user:
      \"用户\"\n    verb: \"操作\"\n    group: \"用户组\"\n    requestReceivedTimestamp:
      \"请求接收时间\"\n    role: \"角色\"\n    host_ip: \"主机IP\"\n    node: \"节点\"\n    rule_id:
      \"告警规则\"\n    owner_kind: \"目标类型\"\n    workload: \"工作负载\"\n"
  kind: ConfigMap
  metadata:
    annotations:
      meta.helm.sh/release-name: notification-manager
      meta.helm.sh/release-namespace: kubesphere-monitoring-system
    creationTimestamp: "2025-02-10T07:02:49Z"
    labels:
      app.kubernetes.io/managed-by: Helm
    name: zh-cn
    namespace: kubesphere-monitoring-system
    resourceVersion: "1023454"
    uid: 2718cb24-3824-4d0d-bc1f-629971723f77
kind: List
metadata:
  resourceVersion: ""
